{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv4FfpnZcRJFrmNtViG+Ou",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SattuSupCodes/SentimentAnalysis_TheoreticalLearning/blob/main/frustrating_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ski2U2KPXdC1",
        "outputId": "16498887-66b2-42ba-959b-9de4008699ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.0.1+cpu (from versions: 2.2.0+cpu, 2.2.1+cpu, 2.2.2+cpu, 2.3.0+cpu, 2.3.1+cpu, 2.4.0+cpu, 2.4.1+cpu, 2.5.0+cpu, 2.5.1+cpu, 2.6.0+cpu, 2.7.0+cpu, 2.7.1+cpu, 2.8.0+cpu, 2.9.0+cpu, 2.9.1+cpu)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.0.1+cpu\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: speechbrain==0.5.16 in /usr/local/lib/python3.12/dist-packages (0.5.16)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (1.2.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (25.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (0.2.1)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (0.36.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->speechbrain==0.5.16) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->speechbrain==0.5.16) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->speechbrain==0.5.16) (1.2.0)\n",
            "Requirement already satisfied: ruamel.yaml<0.19.0,>=0.17.28 in /usr/local/lib/python3.12/dist-packages (from hyperpyyaml->speechbrain==0.5.16) (0.18.17)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.15 in /usr/local/lib/python3.12/dist-packages (from ruamel.yaml<0.19.0,>=0.17.28->hyperpyyaml->speechbrain==0.5.16) (0.2.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.9->speechbrain==0.5.16) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.9->speechbrain==0.5.16) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->speechbrain==0.5.16) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->speechbrain==0.5.16) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->speechbrain==0.5.16) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->speechbrain==0.5.16) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-cache-dir \\\n",
        "  \"torch==2.0.1+cpu\" \\\n",
        "  \"torchaudio==2.0.2+cpu\" \\\n",
        "  --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "!pip install \"speechbrain==0.5.16\" soundfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GWWxwCaFn5RG",
        "outputId": "ad67c902-0465-4e24-c410-6388c86ab289"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import torchaudio\n",
        "import torch\n",
        "import soundfile as sf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "from speechbrain.lobes.models.ECAPA_TDNN import ECAPA_TDNN\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import whisper"
      ],
      "metadata": {
        "id": "ZoURyOEAXx4o"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "ecapa = EncoderClassifier.from_hparams(\n",
        "    source = \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    run_opts = {\"device\": device}\n",
        ")\n",
        "ecapa.eval()\n",
        "print(\"ECAPA loaded (frozen embedding extractor)\") #we downloaded our pretraind ecapa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxYveXNVYWJS",
        "outputId": "c62bdf37-4f37-4e39-99cb-df4ec116a804"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECAPA loaded (frozen embedding extractor)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wav, sr = sf.read(\"sample11.wav\")\n",
        "wav = torch.tensor(wav, dtype=torch.float32)\n",
        "if wav.ndim == 2:\n",
        "  wav = wav.mean(dim=1)\n",
        "print(wav.shape, sr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnIpZa-BZF8I",
        "outputId": "5d887b60-b700-4558-8587-8bbc9157be31"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1048556]) 44100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"sample11.wav\")\n",
        "\n",
        "segments = result[\"segments\"]\n",
        "for seg in segments:\n",
        "    print(f\"{seg['start']:.2f}-{seg['end']:.2f}: {seg['text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ym0I92ijoPMV",
        "outputId": "3bbdb6e6-f7e0-470d-c494-18482859b1bd"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00-5.76:  That was kind of the thought and whatever and then you hit me and I was like no frickin way\n",
            "5.76-7.16:  he's hitting me right now.\n",
            "7.16-11.20:  So I was like absolutely, this is like perfect, it's the universe just putting it in perfect\n",
            "11.20-12.20:  timing because-\n",
            "12.20-15.80:  That makes me so happy because it was exactly the opposite way.\n",
            "15.80-20.40:  So I've for a long time had people that I'd love to sit down with and every time I saw\n",
            "20.40-23.80:  you do something with mental health and vogue or any of the segments-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Since ONLY ECAPA is gving us he results, lets add MFCC layer"
      ],
      "metadata": {
        "id": "hPkdulOkpmeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y, sr = librosa.load(\"sample11.wav\", sr=None)"
      ],
      "metadata": {
        "id": "ZNdJ2OWDp2De"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_len = int(1.5 * sr)\n",
        "segments = [\n",
        "    y[i:i+segment_len]\n",
        "    for i in range(0, len(y), segment_len)\n",
        "    if len(y[i:i+segment_len]) == segment_len\n",
        "]\n"
      ],
      "metadata": {
        "id": "_kQnalNzqCfM"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_len_sec = 1.5\n",
        "segment_len = int(segment_len_sec * sr)\n",
        "\n",
        "segments_audio = [\n",
        "    y[i:i+segment_len]\n",
        "    for i in range(0, len(y), segment_len)\n",
        "    if len(y[i:i+segment_len]) == segment_len\n",
        "]\n"
      ],
      "metadata": {
        "id": "KLqXWEdGqFZp"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mfcc(seg, sr):\n",
        "    mfcc = librosa.feature.mfcc(y=seg, sr=sr, n_mfcc=13)\n",
        "    return np.mean(mfcc, axis=1)\n",
        "\n",
        "X = np.array([extract_mfcc(seg, sr) for seg in segments])\n"
      ],
      "metadata": {
        "id": "wCml7-j6qJdN"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mfcc_embed(seg, sr, n_mfcc=13):\n",
        "    mfcc = librosa.feature.mfcc(y=seg, sr=sr, n_mfcc=n_mfcc)\n",
        "    delta = librosa.feature.delta(mfcc)\n",
        "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "\n",
        "    features = np.concatenate([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        np.mean(delta, axis=1),\n",
        "        np.mean(delta2, axis=1)\n",
        "    ]) #lets just try making our mfcc stronger\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "O3e8XJK3qOS3"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pitch_stats(seg, sr):\n",
        "    pitch = librosa.yin(seg, fmin=50, fmax=400, sr=sr)\n",
        "    pitch = pitch[~np.isnan(pitch)]\n",
        "    if len(pitch) == 0:\n",
        "        return [0, 0]\n",
        "    return [np.mean(pitch), np.std(pitch)]"
      ],
      "metadata": {
        "id": "Glnc5zX_rJZi"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_embed(seg, sr):\n",
        "    mfcc_feat = mfcc_embed(seg, sr)\n",
        "    pitch_feat = pitch_stats(seg, sr)\n",
        "    return np.concatenate([mfcc_feat, pitch_feat])\n"
      ],
      "metadata": {
        "id": "G4k1OSo9rL1n"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([full_embed(seg, sr) for seg in segments_audio])\n",
        "X = StandardScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "3aFjy0HTrU1l"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if segments_audio and len(segments_audio) > 0:\n",
        "    # Use the mfcc_embed function defined earlier with a valid segment\n",
        "    # For demonstration, we use the first segment from segments_audio\n",
        "    sample_seg = segments_audio[0]\n",
        "    mfcc_feat = mfcc_embed(sample_seg, sr)\n",
        "    print(\"MFCC features for a sample segment:\", mfcc_feat.shape)\n",
        "else:\n",
        "    print(\"No audio segments available to extract MFCC features.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVWev0gosn85",
        "outputId": "35f27232-72ac-4124-b158-0f549a1fc83b"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MFCC features for a sample segment: (39,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MFCC ENDS"
      ],
      "metadata": {
        "id": "pBHvRE5ZrmKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if sr!=16000:\n",
        "  wav = torchaudio.functional.resample(wav,sr,16000)\n",
        "  sr=16000"
      ],
      "metadata": {
        "id": "fTgFha3fZ4cu"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sr)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5vBQBqrakml",
        "outputId": "d52af230-56ea-4d07-a0b4-bfc3d247f3c4"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frame_sec = 0.025\n",
        "frame_len = int(frame_sec*sr)\n",
        "hop_len = frame_len//2"
      ],
      "metadata": {
        "id": "2Z05uOFpasPB"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "energies = []\n",
        "\n",
        "for i in range(0, len(wav)-frame_len, hop_len):\n",
        "  frame = wav[i:i+frame_len]\n",
        "  energy = np.sum(frame.numpy()**2)\n",
        "  energies.append(energy)\n",
        "energies = np.array(energies)"
      ],
      "metadata": {
        "id": "lGzIwRLGa5pE"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "energy_thresh = 0.03*np.max(energies)\n",
        "speech_mask = energies>energy_thresh"
      ],
      "metadata": {
        "id": "aLr2Znj9bVuQ"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = []\n",
        "start = None\n",
        "\n",
        "for i, is_speech in enumerate(speech_mask):\n",
        "    t = i * hop_len / sr\n",
        "\n",
        "    if is_speech and start is None:\n",
        "        start = t\n",
        "\n",
        "    elif not is_speech and start is not None:\n",
        "        end = t\n",
        "        if end - start > 0.5:\n",
        "            segments.append((start, end))\n",
        "        start = None\n",
        "\n",
        "\n",
        "if start is not None:\n",
        "    end = len(wav) / sr\n",
        "    if end - start > 0.5:\n",
        "        segments.append((start, end))\n",
        "\n",
        "print(\"Segments created:\", len(segments))\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URoDPvOCbh8A",
        "outputId": "7cbadf8b-225b-41f3-ef53-252b1c05ca41"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segments created: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_sec = 1.0\n",
        "hop_sec = 0.02\n",
        "\n",
        "win_len = int(window_sec * sr)\n",
        "hop_len = int(hop_sec * sr)\n",
        "\n",
        "embeddings = []\n",
        "features = []\n",
        "for start, end in segments:\n",
        "    s = int(start * sr)\n",
        "    e = int(end * sr)\n",
        "    speech = wav[s:e]          # NOW speech is waveform\n",
        "\n",
        "    if len(speech) < win_len:\n",
        "        continue\n",
        "\n",
        "    for i in range(0, len(speech) - win_len + 1, hop_len):\n",
        "        chunk = speech[i:i+win_len]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            emb = ecapa.encode_batch(chunk.unsqueeze(0))\n",
        "\n",
        "        emb = emb.squeeze(0)\n",
        "        embeddings.append(emb.cpu().numpy())\n",
        "embeddings = np.vstack(embeddings)\n",
        "print(embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "collapsed": true,
        "id": "s3YRF_hEc7Pe",
        "outputId": "9c300c5a-7976-48e3-8eda-b956ef9ec621"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "need at least one array to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2834121182.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of VAD segments:\", len(segments))\n",
        "print(\"First 10 VAD segments (sec):\", segments[:10])\n",
        "\n",
        "durations = [(end - start) for start, end in segments]\n",
        "print(\"First 10 segment durations:\", durations[:10])\n",
        "print(\"Max segment duration:\", max(durations) if durations else 0)\n"
      ],
      "metadata": {
        "id": "5bV-hmumkPZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(emb), emb.shape)"
      ],
      "metadata": {
        "id": "QTi1vIgekTCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer = AgglomerativeClustering(\n",
        "    n_clusters=None,\n",
        "    metric=\"cosine\",\n",
        "    linkage=\"average\",\n",
        "    distance_threshold=0.3\n",
        ")\n",
        "\n",
        "labels = clusterer.fit_predict(embeddings)\n",
        "print(\"Speakers detected:\", len(set(labels)))"
      ],
      "metadata": {
        "id": "chtZO0OEkWD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_feature = np.concatenate([emb.cpu().numpy().flatten(), mfcc_feat])\n",
        "# shape: (231,)"
      ],
      "metadata": {
        "id": "Wq3IjmD5tIKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.append(segment_feature)\n"
      ],
      "metadata": {
        "id": "2pPfvGh2tXlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.vstack(features)\n",
        "print(X.shape)\n"
      ],
      "metadata": {
        "id": "cR3WH9zUthoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = StandardScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "8rNdtcYQvm-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer = AgglomerativeClustering(\n",
        "    n_clusters=None,\n",
        "    distance_threshold=0.65,\n",
        "    metric=\"cosine\",\n",
        "    linkage=\"average\"\n",
        ")\n",
        "\n",
        "labels = clusterer.fit_predict(X)\n",
        "print(\"Speakers detected:\", len(set(labels)))\n"
      ],
      "metadata": {
        "id": "ThyALcufvtjZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}