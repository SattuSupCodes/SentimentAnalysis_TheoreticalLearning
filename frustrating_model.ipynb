{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMnmH9MIGTqagH8bYAF7/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SattuSupCodes/SentimentAnalysis_TheoreticalLearning/blob/main/frustrating_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ski2U2KPXdC1",
        "outputId": "16498887-66b2-42ba-959b-9de4008699ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.0.1+cpu (from versions: 2.2.0+cpu, 2.2.1+cpu, 2.2.2+cpu, 2.3.0+cpu, 2.3.1+cpu, 2.4.0+cpu, 2.4.1+cpu, 2.5.0+cpu, 2.5.1+cpu, 2.6.0+cpu, 2.7.0+cpu, 2.7.1+cpu, 2.8.0+cpu, 2.9.0+cpu, 2.9.1+cpu)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.0.1+cpu\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: speechbrain==0.5.16 in /usr/local/lib/python3.12/dist-packages (0.5.16)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (1.2.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (25.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (0.2.1)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from speechbrain==0.5.16) (0.36.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9->speechbrain==0.5.16) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->speechbrain==0.5.16) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->speechbrain==0.5.16) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->speechbrain==0.5.16) (1.2.0)\n",
            "Requirement already satisfied: ruamel.yaml<0.19.0,>=0.17.28 in /usr/local/lib/python3.12/dist-packages (from hyperpyyaml->speechbrain==0.5.16) (0.18.17)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.15 in /usr/local/lib/python3.12/dist-packages (from ruamel.yaml<0.19.0,>=0.17.28->hyperpyyaml->speechbrain==0.5.16) (0.2.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.9->speechbrain==0.5.16) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.9->speechbrain==0.5.16) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->speechbrain==0.5.16) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->speechbrain==0.5.16) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->speechbrain==0.5.16) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->speechbrain==0.5.16) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-cache-dir \\\n",
        "  \"torch==2.0.1+cpu\" \\\n",
        "  \"torchaudio==2.0.2+cpu\" \\\n",
        "  --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "!pip install \"speechbrain==0.5.16\" soundfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GWWxwCaFn5RG",
        "outputId": "ad67c902-0465-4e24-c410-6388c86ab289"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "import torchaudio\n",
        "import torch\n",
        "import soundfile as sf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "from speechbrain.lobes.models.ECAPA_TDNN import ECAPA_TDNN\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import whisper"
      ],
      "metadata": {
        "id": "ZoURyOEAXx4o"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "ecapa = EncoderClassifier.from_hparams(\n",
        "    source = \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    run_opts = {\"device\": device}\n",
        ")\n",
        "ecapa.eval()\n",
        "print(\"ECAPA loaded (frozen embedding extractor)\") #we downloaded our pretraind ecapa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxYveXNVYWJS",
        "outputId": "c62bdf37-4f37-4e39-99cb-df4ec116a804"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECAPA loaded (frozen embedding extractor)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wav, sr = sf.read(\"sample11.wav\")\n",
        "wav = torch.tensor(wav, dtype=torch.float32)\n",
        "if wav.ndim == 2:\n",
        "  wav = wav.mean(dim=1)\n",
        "print(wav.shape, sr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnIpZa-BZF8I",
        "outputId": "5d887b60-b700-4558-8587-8bbc9157be31"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1048556]) 44100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"sample11.wav\")\n",
        "\n",
        "segments = result[\"segments\"]\n",
        "for seg in segments:\n",
        "    print(f\"{seg['start']:.2f}-{seg['end']:.2f}: {seg['text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ym0I92ijoPMV",
        "outputId": "b8d7db41-c946-4c5b-f4ed-6797a5d6da9b"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00-5.52:  Just that was kind of the thought and whatever and then you hit me and I was like no frickin way\n",
            "5.52-9.40:  He's hitting me right now. So I was like absolutely. This is like perfect\n",
            "9.40-15.50:  It's the universe just putting it in perfect timing because that makes me so happy because it was exactly the opposite way\n",
            "15.50-21.76:  So I've for a long time had people that I'd love to sit down with and every time I saw you do something with mental health and\n",
            "21.76-26.54:  Vogue or any of the segments that you ever did I was just like she's amazing like you know\n",
            "26.54-29.66:  It's incredible to the way you were talking about it when you'd post about it\n",
            "29.66-33.82:  I'd be like oh this is incredible like this is see you speaking so openly about it\n",
            "33.82-39.74:  Invulnerably about it and so I've always wanted to do this and then but we've never really crossed paths\n",
            "39.74-44.54:  And so I'm always just a bit like and I don't like asking friends of friends when I haven't met someone and right\n",
            "44.54-47.22:  So I was like that day I had to like pluck up the card\n",
            "47.22-52.30:  I'm just gonna ask of myself no it's easier when I'm not starting and I know Christie's so yeah\n",
            "52.30-56.86:  Yeah, yeah, but I was like I it's nicer when there's I told my team too. I was like it's so much nicer when there's\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Since ONLY ECAPA is gving us he results, lets add MFCC layer"
      ],
      "metadata": {
        "id": "hPkdulOkpmeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y, sr = librosa.load(\"sample11.wav\", sr=None)"
      ],
      "metadata": {
        "id": "ZNdJ2OWDp2De"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_len = int(1.5 * sr)\n",
        "segments = [\n",
        "    y[i:i+segment_len]\n",
        "    for i in range(0, len(y), segment_len)\n",
        "    if len(y[i:i+segment_len]) == segment_len\n",
        "]\n"
      ],
      "metadata": {
        "id": "_kQnalNzqCfM"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_len_sec = 1.5\n",
        "segment_len = int(segment_len_sec * sr)\n",
        "\n",
        "segments_audio = [\n",
        "    y[i:i+segment_len]\n",
        "    for i in range(0, len(y), segment_len)\n",
        "    if len(y[i:i+segment_len]) == segment_len\n",
        "]\n"
      ],
      "metadata": {
        "id": "KLqXWEdGqFZp"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mfcc(seg, sr):\n",
        "    mfcc = librosa.feature.mfcc(y=seg, sr=sr, n_mfcc=13)\n",
        "    return np.mean(mfcc, axis=1)\n",
        "\n",
        "X = np.array([extract_mfcc(seg, sr) for seg in segments])\n"
      ],
      "metadata": {
        "id": "wCml7-j6qJdN"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mfcc_embed(seg, sr, n_mfcc=13):\n",
        "    mfcc = librosa.feature.mfcc(y=seg, sr=sr, n_mfcc=n_mfcc)\n",
        "    delta = librosa.feature.delta(mfcc)\n",
        "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "\n",
        "    features = np.concatenate([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        np.mean(delta, axis=1),\n",
        "        np.mean(delta2, axis=1)\n",
        "    ]) #lets just try making our mfcc stronger\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "O3e8XJK3qOS3"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pitch_stats(seg, sr):\n",
        "    pitch = librosa.yin(seg, fmin=50, fmax=400, sr=sr)\n",
        "    pitch = pitch[~np.isnan(pitch)]\n",
        "    if len(pitch) == 0:\n",
        "        return [0, 0]\n",
        "    return [np.mean(pitch), np.std(pitch)]"
      ],
      "metadata": {
        "id": "Glnc5zX_rJZi"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_embed(seg, sr):\n",
        "    mfcc_feat = mfcc_embed(seg, sr)\n",
        "    pitch_feat = pitch_stats(seg, sr)\n",
        "    return np.concatenate([mfcc_feat, pitch_feat])\n"
      ],
      "metadata": {
        "id": "G4k1OSo9rL1n"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([full_embed(seg, sr) for seg in segments_audio])\n",
        "X = StandardScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "3aFjy0HTrU1l"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if segments_audio and len(segments_audio) > 0:\n",
        "    # Use the mfcc_embed function defined earlier with a valid segment\n",
        "    # For demonstration, we use the first segment from segments_audio\n",
        "    sample_seg = segments_audio[0]\n",
        "    mfcc_feat = mfcc_embed(sample_seg, sr)\n",
        "    print(\"MFCC features for a sample segment:\", mfcc_feat.shape)\n",
        "else:\n",
        "    print(\"No audio segments available to extract MFCC features.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVWev0gosn85",
        "outputId": "f5bc8375-6188-47e5-c24f-7bb7e614e773"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MFCC features for a sample segment: (39,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MFCC ENDS"
      ],
      "metadata": {
        "id": "pBHvRE5ZrmKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if sr!=16000:\n",
        "  wav = torchaudio.functional.resample(wav,sr,16000)\n",
        "  sr=16000"
      ],
      "metadata": {
        "id": "fTgFha3fZ4cu"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sr)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5vBQBqrakml",
        "outputId": "7d839cf0-8409-4ee0-817f-c182c67ff7c8"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frame_sec = 0.025\n",
        "frame_len = int(frame_sec*sr)\n",
        "hop_len = frame_len//2"
      ],
      "metadata": {
        "id": "2Z05uOFpasPB"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "energies = []\n",
        "\n",
        "for i in range(0, len(wav)-frame_len, hop_len):\n",
        "  frame = wav[i:i+frame_len]\n",
        "  energy = np.sum(frame.numpy()**2)\n",
        "  energies.append(energy)\n",
        "energies = np.array(energies)"
      ],
      "metadata": {
        "id": "lGzIwRLGa5pE"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "energy_thresh = 0.03*np.max(energies)\n",
        "speech_mask = energies>energy_thresh"
      ],
      "metadata": {
        "id": "aLr2Znj9bVuQ"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = []\n",
        "start = None\n",
        "\n",
        "for i, is_speech in enumerate(speech_mask):\n",
        "    t = i * hop_len / sr\n",
        "\n",
        "    if is_speech and start is None:\n",
        "        start = t\n",
        "\n",
        "    elif not is_speech and start is not None:\n",
        "        end = t\n",
        "        if end - start > 0.5:\n",
        "            segments.append((start, end))\n",
        "        start = None\n",
        "\n",
        "\n",
        "if start is not None:\n",
        "    end = len(wav) / sr\n",
        "    if end - start > 0.5:\n",
        "        segments.append((start, end))\n",
        "\n",
        "print(\"Segments created:\", len(segments))\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URoDPvOCbh8A",
        "outputId": "d040bc9a-6e24-4a28-f2dc-5f0ded897321"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segments created: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_sec = 1.0 # Window length for ECAPA embedding\n",
        "hop_sec = 0.02 # Hop length for sliding window over speech segments\n",
        "\n",
        "win_len = int(window_sec * sr)\n",
        "feat_hop_len = int(hop_sec * sr)\n",
        "\n",
        "embeddings = []\n",
        "features = [] # Retain the features list as it was present in the original cell\n",
        "\n",
        "# --- Re-execute VAD segment creation with a modified threshold ---\n",
        "# VAD parameters from upstream cells (kernel state):\n",
        "vad_hop_len = 200  # From cell 2Z05uOFpasPB: frame_len // 2\n",
        "vad_sr = 16000     # From cell _5vBQBqrakml: resampled sr\n",
        "\n",
        "segments_for_extraction = [] # Use a new list to avoid conflict with the original 'segments' var\n",
        "current_start = None\n",
        "min_segment_duration_sec = 0.1 # Reduced from 0.5 seconds to capture shorter speech bursts\n",
        "\n",
        "for i, is_speech in enumerate(speech_mask):\n",
        "    t = i * vad_hop_len / vad_sr\n",
        "\n",
        "    if is_speech and current_start is None:\n",
        "        current_start = t\n",
        "\n",
        "    elif not is_speech and current_start is not None:\n",
        "        end = t\n",
        "        if end - current_start > min_segment_duration_sec:\n",
        "            segments_for_extraction.append((current_start, end))\n",
        "        current_start = None\n",
        "\n",
        "# Handle case where speech ends at the very end of the audio\n",
        "if current_start is not None:\n",
        "    end = len(wav) / vad_sr\n",
        "    if end - current_start > min_segment_duration_sec:\n",
        "        segments_for_extraction.append((current_start, end))\n",
        "\n",
        "# Fallback if no segments are detected even with reduced threshold\n",
        "if not segments_for_extraction:\n",
        "    print(\"Warning: No speech segments detected even with reduced threshold. Using entire audio as a single segment.\")\n",
        "    segments_for_extraction = [(0, len(wav) / vad_sr)]\n",
        "\n",
        "\n",
        "# --- Feature extraction loop ---\n",
        "for start, end in segments_for_extraction: # Iterate over the newly created segments\n",
        "    s = int(start * sr)\n",
        "    e = int(end * sr)\n",
        "    speech = wav[s:e]\n",
        "\n",
        "    if len(speech) < win_len:\n",
        "        continue # Skip segments shorter than the embedding window\n",
        "\n",
        "    for i in range(0, len(speech) - win_len + 1, feat_hop_len):\n",
        "        chunk = speech[i:i+win_len]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            emb = ecapa.encode_batch(chunk.unsqueeze(0))\n",
        "\n",
        "        emb = emb.squeeze(0)\n",
        "        embeddings.append(emb.cpu().numpy())\n",
        "\n",
        "# Only stack if embeddings list is not empty to prevent ValueError\n",
        "if embeddings:\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    print(embeddings.shape)\n",
        "else:\n",
        "    print(\"No embeddings could be generated from any segments after processing.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s3YRF_hEc7Pe",
        "outputId": "a7c1a3ea-4701-4ee9-9d26-25aa6c95c25a"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No embeddings could be generated from any segments after processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of VAD segments:\", len(segments))\n",
        "print(\"First 10 VAD segments (sec):\", segments[:10])\n",
        "\n",
        "durations = [(end - start) for start, end in segments]\n",
        "print(\"First 10 segment durations:\", durations[:10])\n",
        "print(\"Max segment duration:\", max(durations) if durations else 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bV-hmumkPZP",
        "outputId": "7ccda475-9223-4ae3-dad9-d2f7054d75c4"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of VAD segments: 0\n",
            "First 10 VAD segments (sec): []\n",
            "First 10 segment durations: []\n",
            "Max segment duration: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(emb), emb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTi1vIgekTCG",
        "outputId": "485b0bdf-7e11-49fd-b88e-2d43761861f8"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> torch.Size([1, 192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer = AgglomerativeClustering(\n",
        "    n_clusters=None,\n",
        "    metric=\"cosine\",\n",
        "    linkage=\"average\",\n",
        "    distance_threshold=0.3\n",
        ")\n",
        "\n",
        "labels = clusterer.fit_predict(embeddings)\n",
        "print(\"Speakers detected:\", len(set(labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "chtZO0OEkWD5",
        "outputId": "e754becb-c60a-4920-cb49-02933f84a8ff"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2570610411.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Speakers detected:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/cluster/_agglomerative.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mCluster\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \"\"\"\n\u001b[0;32m-> 1114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/cluster/_agglomerative.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \"\"\"\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1091\u001b[0m                         \u001b[0;34m\"if it contains a single sample.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                     )\n\u001b[0;32m-> 1093\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kind\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"USV\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment_feature = np.concatenate([emb.cpu().numpy().flatten(), mfcc_feat])\n",
        "# shape: (231,)"
      ],
      "metadata": {
        "id": "Wq3IjmD5tIKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.append(segment_feature)\n"
      ],
      "metadata": {
        "id": "2pPfvGh2tXlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.vstack(features)\n",
        "print(X.shape)\n"
      ],
      "metadata": {
        "id": "cR3WH9zUthoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = StandardScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "8rNdtcYQvm-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer = AgglomerativeClustering(\n",
        "    n_clusters=None,\n",
        "    distance_threshold=0.65,\n",
        "    metric=\"cosine\",\n",
        "    linkage=\"average\"\n",
        ")\n",
        "\n",
        "labels = clusterer.fit_predict(X)\n",
        "print(\"Speakers detected:\", len(set(labels)))\n"
      ],
      "metadata": {
        "id": "ThyALcufvtjZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}